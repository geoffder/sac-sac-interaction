{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dramatic-tunisia",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# local\n",
    "from utils import *\n",
    "sys.path.append(\"../imaging-utils\")\n",
    "from image_arrays import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-migration",
   "metadata": {},
   "source": [
    "### Activate interactive plotting\n",
    "By default, inline plots are static. Here we specify one of two options (comment out the undesired command) that will open plots with GUI controls for us.\n",
    "- **qt ->** figures opened in windows outside the notebook\n",
    "- **notebook ->** figures within notebook underneath generating cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "color-education",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib qt \n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "qualified-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/mnt/Data/prerna_velocity/\"\n",
    "data_path = os.path.join(base_path, \"2021_05_11\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "whole-explosion",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_data = {\n",
    "    depth: {\n",
    "        int(vel): np.stack(\n",
    "            [\n",
    "                io.imread(os.path.join(data_path, depth, vel, f))\n",
    "                for f in os.listdir(os.path.join(data_path, depth, vel))\n",
    "                if (f.endswith(\".tiff\") or f.endswith(\".tif\"))\n",
    "            ],\n",
    "            axis=0\n",
    "        )\n",
    "        for vel in os.listdir(os.path.join(data_path, depth))\n",
    "        if (os.path.isdir(os.path.join(data_path, depth, vel)) and vel.isnumeric())        \n",
    "    }\n",
    "    for depth in [\"DD\", \"PD\"]\n",
    "}\n",
    "\n",
    "vel_df = {\n",
    "    d: {\n",
    "        v: ((trials - np.mean(trials[:, 500:2500], axis=1, keepdims=True)) \n",
    "            / (np.mean(trials[:, 500:2500], axis=1, keepdims=True) + 0.00001))\n",
    "        for v, trials in vels.items()\n",
    "    }\n",
    "    for d, vels in vel_data.items()\n",
    "}\n",
    "\n",
    "n_trials, n_frames, y_sz, x_sz = vel_data[\"DD\"][300].shape\n",
    "dt = 1 / 565\n",
    "recs_xaxis = np.arange(n_frames) * dt\n",
    "velocities = np.array(list(sorted(vel_data[\"DD\"].keys())))\n",
    "print(\"velocity stack shape:\", vel_data[\"DD\"][300].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-bundle",
   "metadata": {},
   "source": [
    "### Dynamic ROI plot of a scan (of particular `depth`) during stimulation by spot moving at `vel` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fantastic-detail",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = 300\n",
    "depth = \"PD\"\n",
    "\n",
    "stack_plot = StackExplorer(\n",
    "    vel_data[depth][vel],\n",
    "    zaxis=recs_xaxis,\n",
    "    delta=5,\n",
    "    roi_sz=(32, 8),\n",
    "    vmin=0,\n",
    "    figsize=(6, 8)\n",
    ")\n",
    "stack_plot.ax[0].set_aspect(20)\n",
    "stack_plot.ax[1].set_xlabel(\"Time (s)\")\n",
    "stack_plot.ax[1].set_ylabel(\"Pixel Value\")\n",
    "stack_plot.fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jewish-services",
   "metadata": {},
   "source": [
    "### Grid ROI placement\n",
    "Take `grid_w` by `grid_h` beams from the scan field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gentle-adventure",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_w = 64\n",
    "grid_h = 12\n",
    "\n",
    "def aligned_avg(avg_recs):\n",
    "    filtered = np.stack([savgol_filter(r, 71, 3) for r in avg_recs], axis=0)\n",
    "    max_idxs = np.argmax(filtered, axis=1)\n",
    "    shifts = max_idxs - np.min(max_idxs)\n",
    "    trim = np.max(shifts)\n",
    "    aligned = np.mean(\n",
    "        [r[s:(-trim + s) if s != trim else None] for r, s in zip(avg_recs, shifts)],\n",
    "        axis=0\n",
    "    )\n",
    "    return aligned\n",
    "    \n",
    "grid_recs, avg_grid_recs, aligned_grid_avg, grid_locs = {}, {}, {}, {}\n",
    "for depth, vels in vel_data.items():\n",
    "    grid_recs[depth], avg_grid_recs[depth], grid_locs[depth], = {}, {}, {}\n",
    "    aligned_grid_avg[depth] = {}\n",
    "    for v, stack in vels.items():\n",
    "        recs, locs = [], []\n",
    "        for x0 in range(0, x_sz, grid_w):\n",
    "            for y0 in range(0, y_sz, grid_h):\n",
    "                beams = np.mean(stack[:, :, y0:y0 + grid_h, x0:x0 + grid_w], axis=(2, 3))\n",
    "                recs.append(beams)\n",
    "                locs.append([x0, y0])        \n",
    "        grid_recs[depth][v] = np.stack(recs, axis=1)\n",
    "        avg_grid_recs[depth][v] = np.mean(grid_recs[depth][v], axis=0)\n",
    "        grid_locs[depth][v] = np.stack(locs, axis=0)\n",
    "        aligned_grid_avg[depth][v] = aligned_avg(avg_grid_recs[depth][v])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "filled-magnitude",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = \"PD\"\n",
    "vel = 300\n",
    "\n",
    "mean_stack_proj = np.mean(vel_data[depth][vel], axis=(0,1))\n",
    "\n",
    "half_w = grid_w / 2\n",
    "half_h = grid_h / 2\n",
    "grid_fig, grid_ax = plt.subplots(1)\n",
    "\n",
    "grid_ax.imshow(mean_stack_proj, cmap=\"gray\", aspect=20)\n",
    "for (x, y) in grid_locs[depth][vel]:\n",
    "    grid_ax.add_patch(\n",
    "        Rectangle(\n",
    "            (x - .5, y - .5),  # grid offset\n",
    "            grid_w, \n",
    "            grid_h, \n",
    "            fill=False,\n",
    "            color=\"red\",\n",
    "            linewidth=1,\n",
    "            linestyle=\"-\"\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-graphic",
   "metadata": {},
   "source": [
    "### Use PeakExplorer to browse through ROIs (scroll wheel) of from the average scans taken from `depth` during `vel` stimulation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = \"PD\"\n",
    "vel = 300\n",
    "\n",
    "\n",
    "filtered = np.stack(\n",
    "    [savgol_filter(roi, 11, 3) for roi in avg_grid_recs[depth][vel]], \n",
    "    axis=0\n",
    ")\n",
    "\n",
    "peak_explorer = PeakExplorer(\n",
    "    recs_xaxis, \n",
    "#     grid_recs[depth][vel][0],\n",
    "#     avg_grid_recs[depth][vel],\n",
    "    filtered,\n",
    "    prominence=8,\n",
    "    width=2,\n",
    "    tolerance=.5,\n",
    "    distance=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-measurement",
   "metadata": {},
   "source": [
    "### Perform biexponential fits on waveforms from either a particulal grid roi, or the peak aligned average of all the ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complete-comfort",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_idx = 0\n",
    "norm_amp_fitting = False\n",
    "use_aligned = False\n",
    "rec_dict = aligned_grid_avg if use_aligned else avg_grid_recs\n",
    "\n",
    "fits, fitted_recs = {}, {}\n",
    "for depth, vels in rec_dict.items():\n",
    "    fits[depth], fitted_recs[depth] = {}, {}\n",
    "    for v, recs in sorted(vels.items()):\n",
    "        if use_aligned:\n",
    "            y = savgol_filter(recs, 91, 3)\n",
    "        else:\n",
    "            y = savgol_filter(recs[grid_idx], 21, 3)\n",
    "        i0 = find_rise_bsln(y, step=5, bsln_start=500, bsln_end=2500)\n",
    "        y_sub = y[i0:] - y[i0]\n",
    "        y_norm = y_sub / y_sub.max()\n",
    "        x_trunc = np.arange(len(y) - i0) * dt\n",
    "        fitter = BiexpFitter(0.05, 0.055, norm_amp=norm_amp_fitting)\n",
    "        fitter.fit(x_trunc, y_norm)\n",
    "        g = fitter.calc_g(x_trunc)\n",
    "        print(\n",
    "            \"[%s, %i] -> tau1: %.4f; tau2: %.4f\" %\n",
    "            (\n",
    "                depth,\n",
    "                v,\n",
    "                fitter.results.params[\"tau1\"], \n",
    "                fitter.results.params[\"tau2\"]\n",
    "            )\n",
    "        )\n",
    "        fits[depth][v] = {\"g\": g, **fitter.results.params}\n",
    "        fitted_recs[depth][v] = y_norm\n",
    "\n",
    "aligned_tag = \"_aligned\" if use_aligned else \"\"\n",
    "if norm_amp_fitting:\n",
    "    pack_hdf(os.path.join(data_path, \"velocity_tau_fits_normed_peaks\" + aligned_tag), fits)\n",
    "else:\n",
    "    pack_hdf(os.path.join(data_path, \"velocity_tau_fits\" + aligned_tag), fits)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apparent-miniature",
   "metadata": {},
   "outputs": [],
   "source": [
    "def biexp(x, m, t1, t2, b):\n",
    "    return m * (np.exp(-t1 * x) - np.exp(-t2 * x)) + b\n",
    "\n",
    "def mono_exp_decay(x, m, t, b):\n",
    "    return m * np.exp(-t * x) + b\n",
    "\n",
    "fit_tau_fig, fit_tau_ax = plt.subplots(len(velocities), sharex=True, figsize=(7, 8))\n",
    "for i, (ax, vel) in enumerate(zip(fit_tau_ax, velocities)):\n",
    "    for j, depth in enumerate(vel_data.keys()):\n",
    "        x = np.arange(len(fitted_recs[depth][vel])) * dt\n",
    "        norm_fit = fits[depth][vel][\"g\"] / np.max(fits[depth][vel][\"g\"])\n",
    "        ax.plot(x, fitted_recs[depth][vel], c=\"C%i\" % j, label=depth if not i else None)\n",
    "        ax.plot(\n",
    "            x, norm_fit, c=\"C%i\" % j, linestyle=\"--\", \n",
    "            label=((\"%s fit\" % depth) if not i else None)\n",
    "        )\n",
    "        ax.set_title(\"%i μm/s\" % vel)\n",
    "    ax.set_xlim(0, 1.4)\n",
    "\n",
    "ax.set_xlabel(\"Time (s)\")\n",
    "fit_tau_fig.legend()\n",
    "fit_tau_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-worse",
   "metadata": {},
   "source": [
    "### Tau falloffs over velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quality-defeat",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_fig, tune_ax = plt.subplots(2, sharex=True)\n",
    "for ax, (depth, vels) in zip(tune_ax, fits.items()):\n",
    "    tau1 = [vels[v][\"tau1\"] for v in velocities]\n",
    "    tau2 = [vels[v][\"tau2\"] for v in velocities]\n",
    "    t1_coefs = np.polyfit(velocities, tau1, deg=2)\n",
    "    t2_coefs = np.polyfit(velocities, tau2, deg=2)\n",
    "    ax.scatter(velocities, tau1, marker=\"+\", label=\"tau1\")\n",
    "    ax.scatter(velocities, tau2, marker=\"x\", label=\"tau2\")\n",
    "    ax.plot(velocities, np.poly1d(t1_coefs)(velocities))\n",
    "    ax.plot(velocities, np.poly1d(t2_coefs)(velocities))\n",
    "    ax.set_title(depth)\n",
    "\n",
    "for a in tune_ax:\n",
    "    a.set_ylabel(\"Time Constant (s)\")\n",
    "    a.set_ylim(0)\n",
    "    a.legend()\n",
    "    \n",
    "tune_ax[0].set_title(\"DD\")\n",
    "tune_ax[1].set_xlabel(\"Velocity (μm/s)\")\n",
    "tune_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-semester",
   "metadata": {},
   "source": [
    "### Comparison between ROIs (or aligned averages) of DD and PD depths for each velocity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mysterious-breeding",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grid_idxs = {\"DD\": 5, \"PD\": 5}\n",
    "# grid_idxs = {\"DD\": 0, \"PD\": 0}\n",
    "use_aligned = True\n",
    "\n",
    "norm = True\n",
    "depth_comp_fig, depth_comp_ax = plt.subplots(len(velocities), sharex=True, figsize=(6, 8))\n",
    "for ax, vel in zip(depth_comp_ax, velocities):\n",
    "#     if vel == 3000:\n",
    "#         continue\n",
    "    for depth in vel_data.keys():\n",
    "        if use_aligned:\n",
    "            y = savgol_filter(aligned_grid_avg[depth][vel], 21, 3)\n",
    "        else:\n",
    "            y = savgol_filter(avg_grid_recs[depth][vel][grid_idxs[depth]], 11, 3)\n",
    "#         y = avg_grid_recs[depth][vel][3]\n",
    "#         y = savgol_filter(grid_recs[depth][vel][1, 2], 21, 3)\n",
    "        x = np.arange(len(y)) * dt\n",
    "        if norm:\n",
    "            y /= y.max()\n",
    "        ax.plot(x, y, label=depth)\n",
    "        ax.set_title(\"%i μm/s\" % vel)\n",
    "        ax.legend()\n",
    "        ax.set_xlim(4, 5.7)\n",
    "\n",
    "depth_comp_ax[-1].set_xlabel(\"Time (s)\")\n",
    "depth_comp_fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-flash",
   "metadata": {},
   "source": [
    "### Prerna Waveforms, first N (2021_05_11)\n",
    "Waveforms extracted by Prerna, used for powerpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "center-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "waveforms = {d: {v: [] for v in velocities} for d in [\"DD\", \"PD\"]}\n",
    "for f in os.listdir(os.path.join(data_path, \"waveforms2\")):\n",
    "    waves = pd.read_csv(os.path.join(data_path, \"waveforms2\", f), skiprows=1).values.T\n",
    "    for v, w in zip(velocities, waves):\n",
    "        if not np.isnan(w).any():\n",
    "            waveforms[f[:2]][v].append(w)\n",
    "\n",
    "waveforms = {\n",
    "    d: {v: np.stack(ws, axis=0) for v, ws in vels.items()} \n",
    "    for d, vels in waveforms.items()\n",
    "}\n",
    "\n",
    "avg_waveforms = {\n",
    "    d: {v: np.mean(ws, axis=0) for v, ws in vels.items()} \n",
    "    for d, vels in waveforms.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-renaissance",
   "metadata": {},
   "source": [
    "### Manual fitting (bespoke adjustment of taus to line them up by eye)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beneficial-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_taus = {\n",
    "    \"DD\": {\n",
    "        # TODO: fudged 300 and 500 taus to matche the ppt better\n",
    "        300: {\"tau1\": 0.14, \"tau2\": 0.15},\n",
    "        500: {\"tau1\": 0.12, \"tau2\": 0.120001},\n",
    "        1000: {\"tau1\": 0.1, \"tau2\": 0.11},\n",
    "        2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "        3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "        4000: {\"tau1\": 0.030, \"tau2\": 0.031},\n",
    "        5000: {\"tau1\": 0.03, \"tau2\": 0.031},\n",
    "    },\n",
    "    \"PD\": {\n",
    "        # TODO: fudged 300 and 500 taus to matche the ppt better\n",
    "        300: {\"tau1\": 0.45, \"tau2\": 0.4500001},\n",
    "        500: {\"tau1\": 0.3, \"tau2\": 0.30001},\n",
    "        1000: {\"tau1\": 0.15, \"tau2\": 0.150001},\n",
    "        2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "        3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "        4000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "        5000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "    },\n",
    "}\n",
    "# manual_taus = {\n",
    "#     \"DD\": {\n",
    "#         # TODO: fudged 300 and 500 taus to matche the ppt better\n",
    "#         300: {\"tau1\": 0.10, \"tau2\": 0.1001},\n",
    "#         500: {\"tau1\": 0.08, \"tau2\": 0.080001},\n",
    "#         1000: {\"tau1\": 0.07, \"tau2\": 0.0701},\n",
    "#         2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "#         3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "#         4000: {\"tau1\": 0.030, \"tau2\": 0.031},\n",
    "#         5000: {\"tau1\": 0.03, \"tau2\": 0.031},\n",
    "#     },\n",
    "#     \"PD\": {\n",
    "#         # TODO: fudged 300 and 500 taus to matche the ppt better\n",
    "#         300: {\"tau1\": 0.45, \"tau2\": 0.4500001},\n",
    "#         500: {\"tau1\": 0.3, \"tau2\": 0.30001},\n",
    "#         1000: {\"tau1\": 0.15, \"tau2\": 0.150001},\n",
    "#         2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "#         3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "#         4000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "#         5000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "#     },\n",
    "# }\n",
    "\n",
    "dummy = BiexpFitter(0.057, .116, norm_amp=True)\n",
    "\n",
    "manual_tau_fig, manual_tau_ax = plt.subplots(len(velocities), sharex=True, figsize=(7, 8))\n",
    "for i, (ax, vel) in enumerate(zip(manual_tau_ax, velocities)):\n",
    "    for j, depth in enumerate(vel_data.keys()):\n",
    "        x = np.arange(1.5 / dt) * dt\n",
    "        fit = dummy.model(\n",
    "            t=x, \n",
    "            tau1=manual_taus[depth][vel][\"tau1\"],\n",
    "            tau2=manual_taus[depth][vel][\"tau2\"],\n",
    "            y0=1.,\n",
    "        )[0]\n",
    "        manual_taus[depth][vel][\"g\"] = fit\n",
    "        wave = avg_waveforms[depth][vel] / avg_waveforms[depth][vel].max()\n",
    "        if depth == \"DD\":  # use the index from DD\n",
    "            rise_idx = find_rise_bsln(wave, bsln_start=1000, bsln_end=2000)\n",
    "        if not i:\n",
    "            wave_lbl = depth\n",
    "            fit_lbl = \"%s fit\" % depth\n",
    "        else:\n",
    "            wave_lbl, fit_lbl = None, None\n",
    "        ax.plot(x, wave[rise_idx:rise_idx + len(x)], c=\"C%i\" % j, label=wave_lbl)\n",
    "        ax.plot(x, fit, linestyle=\"--\", c=\"C%i\" % j, label=fit_lbl)\n",
    "        ax.set_title(\"%i μm/s\" % vel)\n",
    "        \n",
    "manual_tau_fig.legend()\n",
    "manual_tau_fig.tight_layout()\n",
    "\n",
    "pack_hdf(os.path.join(data_path, \"manual_tau_fits\"), manual_taus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-haven",
   "metadata": {},
   "source": [
    "### Prerna Waveforms, second N (2021_05_12)\n",
    "Waveforms from another experimental day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nasty-founder",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path_2 = os.path.join(base_path, \"2021_05_12\")\n",
    "velocities_2 = np.array([100, 300, 500, 1000, 2000, 3000, 4000, 5000])\n",
    "\n",
    "waveforms_2 = {d: {v: [] for v in velocities_2} for d in [\"DD\", \"PD\"]}\n",
    "for f in os.listdir(os.path.join(data_path_2, \"waveforms\")):\n",
    "    waves = pd.read_csv(os.path.join(data_path_2, \"waveforms\", f), skiprows=1).values.T\n",
    "    for v, w in zip(velocities_2, waves):\n",
    "        if not np.isnan(w).any():\n",
    "            waveforms_2[f[:2]][v].append(w)\n",
    "\n",
    "waveforms_2 = {\n",
    "    d: {v: np.stack(ws, axis=0) for v, ws in vels.items()}\n",
    "    for d, vels in waveforms_2.items()\n",
    "}\n",
    "\n",
    "avg_waveforms_2 = {\n",
    "    d: {v: np.mean(ws, axis=0) for v, ws in vels.items()} \n",
    "    for d, vels in waveforms_2.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "trained-polish",
   "metadata": {},
   "outputs": [],
   "source": [
    "manual_taus_2 = {\n",
    "    \"DD\": {\n",
    "        100: {\"tau1\": 0.15, \"tau2\": 0.16},\n",
    "        300: {\"tau1\": 0.15, \"tau2\": 0.16},\n",
    "        500: {\"tau1\": 0.12, \"tau2\": 0.120001},\n",
    "        1000: {\"tau1\": 0.05, \"tau2\": 0.07},\n",
    "        2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "        3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "        4000: {\"tau1\": 0.030, \"tau2\": 0.031},\n",
    "        5000: {\"tau1\": 0.03, \"tau2\": 0.031},\n",
    "    },\n",
    "    \"PD\": {\n",
    "        100: {\"tau1\": 0.15, \"tau2\": 0.16},\n",
    "        300: {\"tau1\": 0.23, \"tau2\": 0.24},\n",
    "        500: {\"tau1\": 0.15, \"tau2\": 0.17},\n",
    "        1000: {\"tau1\": 0.07, \"tau2\": 0.09},\n",
    "        2000: {\"tau1\": 0.045, \"tau2\": 0.05},\n",
    "        3000: {\"tau1\": 0.035, \"tau2\": 0.04},\n",
    "        4000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "        5000: {\"tau1\": 0.029, \"tau2\": 0.03},\n",
    "    },\n",
    "}\n",
    "\n",
    "# dummy = BiexpFitter(0.057, .116, norm_amp=True)\n",
    "\n",
    "manual_tau_2_fig, manual_tau_2_ax = plt.subplots(\n",
    "    len(velocities_2), sharex=True, figsize=(7, 8))\n",
    "for i, (ax, vel) in enumerate(zip(manual_tau_2_ax, velocities_2)):\n",
    "    for j, depth in enumerate(vel_data.keys()):\n",
    "        x = np.arange(7.5 / dt) * dt\n",
    "#         x = np.arange(2.0 / dt) * dt\n",
    "#         fit = dummy.model(\n",
    "#             t=x, \n",
    "#             tau1=manual_taus_2[depth][vel][\"tau1\"],\n",
    "#             tau2=manual_taus_2[depth][vel][\"tau2\"],\n",
    "#             y0=1.,\n",
    "#         )[0]\n",
    "#         manual_taus_2[depth][vel][\"g\"] = fit\n",
    "        wave = avg_waveforms_2[depth][vel] / avg_waveforms_2[depth][vel][2000:].max()\n",
    "        if depth == \"DD\":  # use the index from DD\n",
    "            rise_idx = find_rise_bsln(wave, bsln_start=1000, bsln_end=2000)\n",
    "        if not i:\n",
    "            wave_lbl = depth\n",
    "            fit_lbl = \"%s fit\" % depth\n",
    "        else:\n",
    "            wave_lbl, fit_lbl = None, None\n",
    "        rise_idx = 0\n",
    "        ax.plot(x, wave[rise_idx:rise_idx + len(x)], c=\"C%i\" % j, label=wave_lbl)\n",
    "#         ax.plot(wave[2000:], c=\"C%i\" % j, label=wave_lbl)\n",
    "#         ax.plot(x, fit, linestyle=\"--\", c=\"C%i\" % j, label=fit_lbl)\n",
    "        ax.set_title(\"%i μm/s\" % vel)\n",
    "        ax.set_xlim(4)\n",
    "\n",
    "manual_tau_2_fig.legend()\n",
    "manual_tau_2_fig.tight_layout()\n",
    "\n",
    "pack_hdf(os.path.join(data_path_2, \"manual_tau_fits\"), manual_taus_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lasting-discovery",
   "metadata": {},
   "source": [
    "### Comparison between ROIs of the same scan (consistent?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-elizabeth",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = 300\n",
    "align = True\n",
    "keep_rois = None\n",
    "# keep_rois = {0, 3}\n",
    "\n",
    "roi_fall_fig, roi_fall_ax = plt.subplots(2, sharex=True, figsize=(8, 8))\n",
    "for a, depth in zip(roi_fall_ax, [\"DD\", \"PD\"]):\n",
    "    for i, roi in enumerate(avg_grid_recs[depth][vel]):\n",
    "        if keep_rois is not None and i not in keep_rois:\n",
    "            continue\n",
    "        w = savgol_filter(roi, 21, 3)\n",
    "        w -= np.mean(w[1000:2000])\n",
    "        w /= np.max(w)\n",
    "        if align:\n",
    "            x = (np.arange(len(w)) - np.argmax(w)) * dt\n",
    "        else:    \n",
    "            x = np.arange(len(w)) * dt\n",
    "        a.plot(x, w, label=(\"roi %i\" % i) if depth == \"DD\" else None)\n",
    "    if align:\n",
    "        a.set_xlim(-.5, 1)\n",
    "    else:\n",
    "        a.set_xlim(4, 6)\n",
    "    a.set_title(depth)\n",
    "    \n",
    "a.set_xlabel(\"Time%s(s)\" % (\" relative to peak \" if align else \" \"))\n",
    "roi_fall_fig.legend()\n",
    "roi_fall_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "protective-contrary",
   "metadata": {},
   "source": [
    "### DD vs PD for each ROI of the avg trial scan (same relationship throughout?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = 300\n",
    "align = False\n",
    "sub_bsln = False\n",
    "norm = False\n",
    "# rois = [0, 3]\n",
    "rois = [0, 1, 3, 4, 5, 6, 7]\n",
    "\n",
    "roi_compare_fig, roi_compare_ax = plt.subplots(len(rois), sharex=True, figsize=(8, 8))\n",
    "for a, i in zip(roi_compare_ax, rois):\n",
    "    for depth in [\"DD\", \"PD\"]:\n",
    "        w = savgol_filter(avg_grid_recs[depth][vel][i], 31, 3)\n",
    "        if sub_bsln:\n",
    "            w -= np.mean(w[1000:2000])\n",
    "        if norm:\n",
    "            w /= np.max(w)\n",
    "        if align:\n",
    "            x = (np.arange(len(w)) - np.argmax(w)) * dt\n",
    "        else:    \n",
    "            x = np.arange(len(w)) * dt\n",
    "        a.plot(x, w, label=depth)\n",
    "    if align:\n",
    "        a.set_xlim(-.5, 1)\n",
    "    else:\n",
    "        a.set_xlim(4, 6)\n",
    "    a.set_title(\"roi %i\" % i)\n",
    "    a.legend()\n",
    "    \n",
    "a.set_xlabel(\"Time (s)\")\n",
    "roi_compare_fig.tight_layout()\n",
    "roi_compare_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legitimate-creator",
   "metadata": {},
   "source": [
    "### Peak, area, and peak-time metrics for each of the ROIs (averaged trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sudden-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = {}\n",
    "for depth, vels in avg_grid_recs.items():\n",
    "    metrics[depth] = {\"peak\": [], \"area\": [], \"peak_time\": []}\n",
    "    for v in velocities:\n",
    "        acc = {k: [] for k in metrics[depth].keys()}\n",
    "        for roi in vels[v]:\n",
    "            filt = savgol_filter(roi, 11, 3)\n",
    "            acc[\"peak\"].append(np.max(filt))\n",
    "            acc[\"area\"].append(np.mean(filt))\n",
    "            acc[\"peak_time\"].append(recs_xaxis[np.argmax(filt)])\n",
    "        for k, v in acc.items():\n",
    "            metrics[depth][k].append(v)\n",
    "    metrics[depth] = {k: np.array(v).T for k, v in metrics[depth].items()}\n",
    "\n",
    "peak_vel_fig, peak_vel_ax = plt.subplots(\n",
    "    len(metrics[\"DD\"]), 2, sharex=True, sharey=\"row\", figsize=(6, 8)\n",
    ")\n",
    "peak_vel_ax = list(map(list, zip(*peak_vel_ax))) # transpose to column major\n",
    "for i, (col, (depth, ms)) in enumerate(zip(peak_vel_ax, metrics.items())):\n",
    "    for j, (ax, (k, v)) in enumerate(zip(col, ms.items())):\n",
    "        for roi_idx, roi in enumerate(v):\n",
    "            ax.plot(velocities, roi / roi[0], marker=\"o\", linestyle=\"--\")\n",
    "            if not j:\n",
    "                col[0].set_title(depth)\n",
    "    \n",
    "        if not i:\n",
    "            ax.set_ylabel(\"%s\" % k)\n",
    "    col[-1].set_xlabel(\"Velocity\")\n",
    "    \n",
    "peak_vel_fig.tight_layout()\n",
    "peak_vel_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-specialist",
   "metadata": {},
   "source": [
    "### Rough estimate of the temporo-spatial offset required to optimize the summation of the proximal and distal inputs (using the average waveforms from Prerna)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel = 300\n",
    "step = 50\n",
    "n = 15\n",
    "lag_sums = np.stack(\n",
    "    [\n",
    "        (avg_waveforms[\"DD\"][vel] \n",
    "         + np.concatenate([avg_waveforms[\"PD\"][vel][i * step:], np.zeros(i * step)]))\n",
    "        for i in range(n)\n",
    "    ]\n",
    ")\n",
    "\n",
    "offsets = np.arange(n) * -step * dt * 1000\n",
    "lag_peaks = np.max(lag_sums, axis=1)\n",
    "\n",
    "lag_sum_fig, lag_sum_ax = plt.subplots(2)\n",
    "\n",
    "for s, off in zip(lag_sums[:5], offsets):\n",
    "    lag_sum_ax[0].plot(recs_xaxis[:-1], s, label=\"prox %ims early\" % off)\n",
    "lag_sum_ax[0].set_xlim(4.2, 5.75)\n",
    "lag_sum_ax[0].set_xlabel(\"Time (s)\")\n",
    "\n",
    "lag_sum_ax[1].plot(offsets, lag_peaks / np.max(lag_peaks))\n",
    "lag_sum_ax[1].set_ylabel(\"Normalized Peak\")\n",
    "lag_sum_ax[1].set_xlabel(\"Proximal Offset (ms)\")\n",
    "\n",
    "lag_sum_fig.suptitle(\"Optimal Proximal -> Distal Timing\")\n",
    "lag_sum_fig.legend()\n",
    "lag_sum_fig.tight_layout()\n",
    "\n",
    "optimal_dist = offsets[np.argmax(lag_peaks)] * vel / -1000\n",
    "print(\"Optimal prox -> dist offset for %i um/s: %.2fum\" % (vel, optimal_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-helmet",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
